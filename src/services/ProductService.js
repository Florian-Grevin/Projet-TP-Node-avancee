const BaseService = require('../core/BaseService');
const { pipeline } = require('stream/promises');
const csv = require('csv-parser');
const { stringify } = require('csv-stringify')
// Import des streams customs
const ProductValidationTransform = require('../streams/ProductValidationTransform');
const ProductBatchInsertWritable = require('../streams/ProductBatchInsertWritable');
const { Readable } = require('stream');
class ProductService extends BaseService {
    constructor(repository) {
        super(repository);
    }
    // M√©thode CRUD h√©rit√©es (findAll, etc.) sont d√©j√† l√† !
    /**
    * Importe des produits depuis un flux de lecture (ex: req ou file stream)
    * @param {Readable} inputStream
    */
    async importProducts(inputStream) {
        const validationTransform = new ProductValidationTransform();
        // ATTENTION : Il faut lui passer le repository pour qu'il puisse sauvegarder !
        const batchInsertWritable = new ProductBatchInsertWritable({
            repository: this.repository.repo,
            batchSize: 500
        });

        await pipeline(
        inputStream,
        csv(), // Convertit le binaire en objets JS bruts
        validationTransform,
        batchInsertWritable
        );
        console.log("‚úÖ Pipeline d'import termin√© avec succ√®s !");
    }
    /**
    * Exporte les produits vers un flux d'√©criture (ex: res)
    * @param {Writable} outputStream
    */
    async exportProducts(outputStream) {
        const repo = this.repository.repo;
        // G√©n√©rateur async pour lire la BDD ligne par ligne (Memory safe)
        async function* productGenerator() {
            let lastId = 0;
            const batchSize = 1000;
            while (true) {
                // Log pour v√©rifier que la pagination se fait bien
                console.log(`üì¶ Fetching batch starting after ID ${lastId}...`);

                const products = await repo.createQueryBuilder("product")
                .select(["product.id", "product.name", "product.price", "product.stock",
                "product.description", "product.isArchived"])
                .where("product.id > :lastId", { lastId })
                .orderBy("product.id", "ASC")
                .take(batchSize)
                .getMany();
                // Si aucun produit n'est retourn√©, on a fini : on sort de la boucle
                if (products.length === 0) break;
                // On "√©met" (yield) chaque produit un par un vers le stream
                for (const product of products) {
                    yield product;
                }
                // On met √† jour le curseur pour le prochain tour
                lastId = products[products.length - 1].id;
            }
        }
        const queryStream = Readable.from(productGenerator());
        const csvTransformer = stringify({
            header: true,
            columns: ["id", "name", "price", "stock", "description", "isArchived"]
        });
        // 3. Ex√©cution du pipeline (connecte Source -> CSV -> Sortie)
        // Le 'await' permet d'attendre que tout le transfert soit termin√© avant de finir la fonction
        await pipeline(
        queryStream,
        csvTransformer,
        outputStream // Ceci est l'objet 'res' pass√© par le contr√¥leur
        );
    }
}
module.exports = ProductService;
